{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f59752d-7410-4816-84b2-6c82a4f9936b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import the geojson file as a dataframe and remove the first 5 rows as they contain only metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb26d737-744e-41cb-b242-2cf89350d452",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('/bronze/Public_EV_Charging_Points_SDCC.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048678eb-4eb8-438d-bb50-22cdb6a02fcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Add an auto-increasing ID column (starts from 0)\n",
    "df = df.withColumn(\"ID\", monotonically_increasing_id())\n",
    "\n",
    "# Drop the first 5 rows\n",
    "df = df.filter(col(\"ID\") > 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639ff1c2-1544-4ba9-bfc8-7d71452d93b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Keep only the **properties** column, flatten the dataframe and drop null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd1be09-082a-4f62-9656-1c4075114522",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keep only the properties column\n",
    "df = df.select(\"properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b828982-567f-4a9b-a167-185a8e3002bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Flatten the dataframe so it looks like a proper table\n",
    "df = df.select(\n",
    "    expr(\"properties.LEA\").alias(\"Town\"),\n",
    "    expr(\"properties.Location\").alias(\"Location\"),\n",
    "    expr(\"properties.Number_of_chargers\"),\n",
    "    expr(\"properties.ObjectID\"),\n",
    "    expr(\"properties.Operator\"),\n",
    "    expr(\"properties.Rating\"),\n",
    "    expr(\"properties.Type\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0502eb-b321-4cd3-aee5-c5a81c9921c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop null rows\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38962feb-d7a6-4525-bea6-3f1a50afa044",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Replace null and blank values in string columns with \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d4f2ca-0cea-49dd-ab2f-c5806c041fa6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    " \n",
    "string_cols = ['Town', 'Location', 'Operator', 'Rating', 'Type']\n",
    "\n",
    "for col_name in string_cols:\n",
    "    df = df.withColumn(col_name, when(\n",
    "        (col(col_name).isNull() | (col(col_name)==\"\")), \n",
    "        lit(\"Unknown\")\n",
    "    ).otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e502322-c959-441c-a4e8-6cea83f4ce91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Extract the left-most word from **Town** before delimiter using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8f99ca-20f0-475b-abcc-6d1e67c1df3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Use regular expression to extract the first word based on multiple delimiters\n",
    "df = df.withColumn(\"Town\", regexp_extract(col(\"Town\"), r\"^[^\\s-_]+\", 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f209554-51ba-4523-806e-be127ab6f485",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Trim string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5fef1d-12bb-4918-ac92-42eaf914670f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "\n",
    "for col_name in string_cols:\n",
    "    df = df.withColumn(col_name, trim(df[col_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6749c1b-1fac-4c4c-a74a-3e74c58fde7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Convert datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32e0aab-1302-4665-971b-0258a885be15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, LongType, ByteType\n",
    "\n",
    "# Convert string columns to StringType\n",
    "for col_name in string_cols:\n",
    "    df = df.withColumn(col_name, col(col_name).cast(StringType()))\n",
    "\n",
    "# Convert ObjectID & Number_of_chargers to LongType & ByteType respectively\n",
    "df = df.withColumn(\"ObjectID\", col(\"ObjectID\").cast(LongType()))\n",
    "df = df.withColumn(\"Number_of_chargers\", col(\"Number_of_chargers\").cast(ByteType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37600e47-8611-4faa-8f36-0c0798110333",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write to silver layer as a delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a922936-7a92-425b-b810-c1ce171620ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"/delta/EV_charging_points_silver\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Cleanse EV charging point data for silver",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
